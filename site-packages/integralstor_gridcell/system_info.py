"""Routines that determine the status of the all the nodes in the system and provide some utilities around them.

This exports:
  get_replacement_node_info - Get a list of replaceable nodes and potential replacement nodes.
  load_system_config - Loads the complete system config and status based on the master.manifest and master.status
  raid_enabled - Currently a dummy but will tell us whether to create a raid on the underlying drives when creating the pools.

"""

import socket, json

from integralstor_common import common, zfs
from integralstor_gridcell import gluster_volumes, gluster_trusted_pools, ctdb


def get_replacement_node_info(si, vil):
  """Returns a dictionary of the possible source and replacement nodes for node replacement.

  si -- The system info dict with the status of the whole system.
  """

  return_dict = None
  try:
    if not si:
      raise Exception('Required parameters not passed')

    src_node_list = []
    dest_node_list = []

    for hostname in si.keys():
      if not si[hostname]["in_cluster"] :
        #Not in the trusted storage pool so no need for any special replacement.
        continue

      if (si[hostname]["node_status"] != 0) or ( si[hostname]["volume_list"]) :
        #Cant replace primary/secondary by the usual method so avoid them.
        if 'gridcell-pri' not in hostname and 'gridcell-sec' not in hostname:
          src_node_list.append(hostname)
      else:
        if 'gridcell-pri' not in hostname and 'gridcell-sec' not in hostname:
          dest_node_list.append(hostname)

    return_dict = {}
    return_dict["src_node_list"] = src_node_list
    return_dict["dest_node_list"] = dest_node_list
  except Exception, e:
    return None, 'Error getting replacement GRIDCell information : %s'%str(e)
  else:
    return return_dict, None

def load_system_config(first_time = False):
  """Returns a dictionary with the configuration and status of all the nodes and their components. This is done by comparing the originally generated master.manifest with a master.status that is generated every minute and combining them into one large dict.

  first_time - This indicates whether it is being called during the initial setup. If so, there is no admin volume that has been created and so no admin volume mountpoint. So we need to pick up the files from a tmp directory.
  """

  return_dict = {}
  try:
    ctdb_status = None
    if first_time:
      system_status_path, err = common.get_tmp_path()
      if err:
        raise Exception(err)
    else:
      system_status_path, err = common.get_system_status_path()
      if err:
        raise Exception(err)

    status_filename = "%s/master.status"%system_status_path
    manifest_filename = "%s/master.manifest"%system_status_path

    with open(status_filename, "r") as f:
      status_nodes = json.load(f)
    with open(manifest_filename, "r") as f:
      manifest_nodes = json.load(f)
  
    # First initialize it with the master nodes 
    return_dict = manifest_nodes

    '''
    #Commenting out as we wont use CTDB for this build
    #Get the CTDB status so we can populate it into each node's dict
    if not first_time:
      ctdb_status, err = ctdb.get_status()
    #print ctdb_status, err
    if err:
      raise Exception(err)
    '''

    #Generate the gluster trusted pool peer list and add the localhost onto it. Need this info to update the cluster status.
    peer_list, err = gluster_trusted_pools.get_peer_list()
    #print 'peer list is ', peer_list
    if err:
      raise Exception(err)
    '''
    #Need to add the localhost because it is never returned as part of the peer list
    localhost = socket.getfqdn().strip()
    tmpd = {}
    tmpd["hostname"] = localhost
    tmpd["status"] = 1
    tmpd["status_str"] = "Peer in Cluster"
    peer_list.append(tmpd)
    '''

    #Get the list of basic info about the volumes. Need this to updates the volumes residing on each node.
    vil, err = gluster_volumes.get_basic_volume_info_all()
    if err:
      raise Exception(err)

    for hostname in return_dict.keys():

      if hostname not in status_nodes:
        #That node is not there in the status so what do we do?? This should never happen..
        continue


      #Default it to not being in the gluster trusted pool
      return_dict[hostname]['cluster_status_str'] = 'Peer not in cluster'
      return_dict[hostname]["in_cluster"] = False


      #Populate the list of volumes on this node.
      return_dict[hostname]["volume_list"], err = gluster_volumes.get_volumes_on_node(hostname, vil)
      if err:
        raise Exception(err)

      #Now start updating it with the status info.
      status_node = status_nodes[hostname]

      for status_node_key in status_node.keys():

        if status_node_key not in return_dict[hostname]:
          #It is a new piece of info from the status dict so add it..
          return_dict[hostname][status_node_key] = status_node[status_node_key]

        elif status_node_key == "disks":
          for disk in status_node["disks"].keys():
            if disk in return_dict[hostname]["disks"]:
              #Disk exists so just update the dict from status..
              return_dict[hostname]["disks"][disk].update(status_node["disks"][disk])
            else:
              #Unknown disk in status. for now pass..
              pass

          #In case we have more than one pool per node which is not the case now, scan through them and determine which pool each disk is part of. This will be used for disk replacement.
          pool_list  = status_node['pools']
          if pool_list:
            for serial_num, disk in return_dict[hostname]['disks'].items():
              id = disk['id']
              #print id
              found = False
              for pool in pool_list:
                devices_list, err = zfs.get_disks_in_component(pool['config']['pool']['root'])
                #print devices_list
                if err:
                  raise Exception(err)
                if devices_list and id in devices_list:
                  disk['pool'] = pool['pool_name']
        elif status_node_key == "interfaces":
          for interface in status_node["interfaces"].keys():
            if interface in return_dict[hostname]["interfaces"]:
              return_dict[hostname]["interfaces"][interface].update(status_node["interfaces"][interface])
        elif status_node_key == "memory":
          #Pull in new memory keys that are not part of the manifest like free, used, etc.
          for mem_key in status_node["memory"].keys():
            if mem_key not in return_dict[hostname]["memory"]:
              return_dict[hostname]["memory"][mem_key] = status_node["memory"][mem_key]

      #Now update the trusted pool status.
      return_dict[hostname]["in_cluster"] = False
      if peer_list:
        for peer in peer_list:
          #print 'peer is ', peer
          #peer hostname could be hostname or IP so need to check both
          if (hostname == peer["hostname"]) or ('inet' in return_dict[hostname]['interfaces']['bond0'] and return_dict[hostname]['interfaces']['bond0']['inet'][0]['address'] == peer['hostname']):
            #print 'updating ', hostname
            return_dict[hostname]["in_cluster"] = True
            return_dict[hostname]["cluster_status"] = int(peer["status"])
            return_dict[hostname]["cluster_status_str"] = peer["status_str"]
            if 'status' in peer:
              if peer['status'] == '1':
                return_dict[hostname]["cluster_status_str"] += ' (Connected)'
              else:
                return_dict[hostname]["cluster_status_str"] += ' (Disconnected)'
            break

      #Store the bond0 IP as a key just for easy reference since we need it a lot.
      if 'interfaces' in return_dict[hostname] and return_dict[hostname]['interfaces'] and 'bond0' in return_dict[hostname]['interfaces'] and return_dict[hostname]['interfaces']['bond0'] and 'inet' in return_dict[hostname]['interfaces']['bond0'] and return_dict[hostname]['interfaces']['bond0']['inet'] and return_dict[hostname]['interfaces']['bond0']['inet'][0] and 'address' in return_dict[hostname]['interfaces']['bond0']['inet'][0] and return_dict[hostname]['interfaces']['bond0']['inet'][0]['address']:
        return_dict[hostname]['bond0_ip'] = return_dict[hostname]['interfaces']['bond0']['inet'][0]['address']

    '''
    #Commenting out as we wont use CTDB for this build
      if ctdb_status:
        for n, v in ctdb_status.items():
          if 'bond0_ip' in return_dict[hostname] and return_dict[hostname]['bond0_ip'] == n:
            return_dict[hostname]['ctdb_status'] = v
            break
    '''

  except Exception, e:
    return None, 'Error loading system configuration : %s'%str(e)
  else:
    return return_dict, None

def get_hostname_given_bond_ip(ip, si = None):
  """Given an IP, check if it is part of the grid and if so, return the hostname of the system for which this IP is configured for bond0

  ip - The IP addres for which the hostname is required
  si - The system info dict if present, if not, we generate it
  """
  hostname = None
  try:
    if not si:
      si, err = load_system_config()
      if err:
        raise Exception(err)
    for h, info in si.items():
      if 'bond0_ip' in info and info['bond0_ip'] == ip:
        hostname = h
        break
  except Exception, e:
    return None, 'Error retrieving hostname given bond IP: %s'%str(e)
  else:
    return hostname, None

def raid_enabled():
  #Enabled by default now. We need to change this to read from the config db which in turn will be updated at the time of first config
  return True, None

def main():

  #print get_chassis_components_status()
  print load_system_config()[0]['gridcell-pri.integralstor.lan']['in_cluster']
  #sl = load_system_config()
  #print "System config :"
  #print sl
  pass


if __name__ == "__main__":
  main()

